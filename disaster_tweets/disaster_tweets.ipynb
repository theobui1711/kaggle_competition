{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  target\n0  Our Deeds are the Reason of this #earthquake M...       1\n1             Forest fire near La Ronge Sask. Canada       1\n2  All residents asked to 'shelter in place' are ...       1\n3  13,000 people receive #wildfires evacuation or...       1\n4  Just got sent this photo from Ruby #Alaska as ...       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "\n",
    "df_train.drop(columns=['id', 'keyword', 'location'], inplace=True)\n",
    "df_test.drop(columns=['id', 'keyword', 'location'], inplace=True)\n",
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "test = df_test\n",
    "\n",
    "train.reset_index(inplace=True)\n",
    "val.reset_index(inplace=True)\n",
    "test.reset_index(inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'text', 'target'],\n",
      "        num_rows: 6090\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['index', 'text', 'target'],\n",
      "        num_rows: 1523\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'text'],\n",
      "        num_rows: 3263\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = DatasetDict()\n",
    "\n",
    "ds['train'] = Dataset.from_pandas(train)\n",
    "ds['validation'] = Dataset.from_pandas(val)\n",
    "ds['test'] = Dataset.from_pandas(test)\n",
    "\n",
    "print(ds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/6090 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9291702986bb46d2aa4cbd4c36c533cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1523 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e479cc505fed4c90bf1a8efe14b96ce6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/3263 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "384f493998f347bcb68bc97044bb1049"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['index', 'text', 'target', 'input_ids', 'attention_mask'],\n        num_rows: 6090\n    })\n    validation: Dataset({\n        features: ['index', 'text', 'target', 'input_ids', 'attention_mask'],\n        num_rows: 1523\n    })\n    test: Dataset({\n        features: ['index', 'text', 'input_ids', 'attention_mask'],\n        num_rows: 3263\n    })\n})"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(batch):\n",
    "    tokenizer_batch = tokenizer(batch['text'], padding=True, truncation=True, max_length=160)\n",
    "    if 'target' in batch:\n",
    "        tokenizer_batch['labels'] = batch['target']\n",
    "    return tokenizer_batch\n",
    "\n",
    "tokenized_ds = ds.map(preprocess, batched=True, batch_size=32)\n",
    "tokenized_ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load('accuracy')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction = trainer.predict(tokenized_ds['validation'])\n",
    "prediction.metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction = trainer.predict(tokenized_ds['test'])\n",
    "prediction.metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def display_confusion_matrix(y_true, y_pred, dataset_name):\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true,\n",
    "        np.argmax(y_pred, axis=1),\n",
    "        display_labels=[\"Not Disaster\",\"Disaster\"],\n",
    "        cmap=plt.cm.Blues\n",
    "    )\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, np.argmax(y_pred, axis=1)).ravel()\n",
    "    f1_score = tp / (tp+((fn+fp)/2))\n",
    "\n",
    "    disp.ax_.set_title(\"Confusion Matrix on \" + dataset_name + \" Dataset -- F1 Score: \" + str(f1_score.round(2)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
